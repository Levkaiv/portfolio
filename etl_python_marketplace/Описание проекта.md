# Автоматизация ETL-процессов для отчётности по артикулам на маркетплейсах

## Описание
В проекте реализован ETL-процесс для автоматизации сбора и подготовки отчётности по продажам и маркетингу на маркетплейсах. Данные интегрируются из SQL-базы, Excel-отчётов и API маркетплейсов, проходят очистку, объединяются и загружаются в BI-инструмент для визуализации ключевых метрик бизнеса.

#### Примечание
В проекте используются искуственно сгенерированные данные, кондифициальность компании, для которой реализован данный проект, сохранена.


## Структура проекта
1) Файл [avto_abc.py](avto_abc.py) содержит основные функции:
- для загрузки данных {extract_data} из API маркетплейсов, 2-х баз данных (PostgreSQL и ClickHouse)
- для обработки данных {ET_wb} и {ET_oz} в отдельности для каждого маркетплейса 
- для загрузки данных {load_wb} и {load_oz} также для каждого маркетплейса своя функция из-за особенностей получаемых данных. Данные загружаются в Google Sheets при помощи библиотеки gspread по столбцам. В файле Google Sheets настроен шаблон данных, в который как раз подгружаеются данных. Шаблон оформил в этом файле: [Шаблон поартикульного анализа](https://docs.google.com/spreadsheets/d/1_5ICv-mX2uhRGFLRci1TAKPeW9Rvc11z7R9TqulGrWo/edit?gid=1360771678#gid=1360771678)

2) Файл [data_input.py](data_input.py) содержит обновляемые даты для формирования новой отчётности. 
Я при формировании полуавтоматизированного отчёта выставлял необходимые даты за новую неделю/месяц и даты использовались для всего ETL-процесса.

3) Файл [main.py](main.py) является запускающимся файлом.  
В нём реализованы запуски всех необходимых функций, написаны принты для просмотра логов ETL-прооцесса и быстрого поиска возможных ошибок.

Также в проекте реализованы функции для загрузки данных по прошлым периодам для сравнению с текущим, так как заказчик требовал реализовать сравнение данных и показать прирост.

Подключение к Базам данных на сервере компании реализовано через ssh тунель. Данные все лежат в виртуальном окружении, которого в данном проекте нет (кондифициальность компании).

### Источники данных для ETL
1) Данные по рекламе из БД компании (SQL запросом получаю)
2) Данные по остаткам из БД компании (SQL запросом получаю)
3) Данные по апи ключам для запросов к API из БД компании (SQL запросом получаю)
4) Финансовые отчёты по WB из локальной БД, на которую записываю данные из скачанных отчётом с сайта WB (финансовые отчёты по API дают некорректную информацию)
5) Справочник артикулов из API я беру с "https://dev.wildberries.ru/openapi/work-with-products#tag/Kartochki-tovarov/paths/~1content~1v2~1get~1cards~1list/post" запроса к API Wildberries.
6) Таким же образом выгружаю данные по платной приёмке с этого запроса: "https://dev.wildberries.ru/openapi/reports#tag/Platnaya-priyomka/paths/~1api~1v1~1acceptance_report~1tasks~1%7Btask_id%7D~1download/get"
7) С Ozon выгружаю данные по товарам с этого запроса: "https://docs.ozon.ru/api/seller/?__rr=2#operation/ProductAPI_GetProductList".
8) Отчёты от маркетплейса WB по организациям из excel файла (оттуда сумму хранения использую в расчётах)
9) Финансовые отчёты от Ozon из excel файла
10) Данные по рекламе на Ozon также из excel файла подгружаю.


### Итог проекта
Отчёт почти полностью автоматизирован. Всё равно скрипт запускал лично я каждую неделю, так как необходимо было отследить верность собранных менеджерами данных по финансовым отчётам от маркетплейсов.

До проекта: отчёт собирался финансовым менеджером (аналитиком) вручную, агрегировались в Excel и загружались очень долго (из-за большого объёма данных висела вся таблица Google Sheets). Подготовка отчёта занимала 1 полный рабочий день.

После проекта: Данные собирались менеджерами за 5-10 минут с каждого кабинета на маркетплейсе. Остальную обработку данных запускал я и за 30 минут (вместе с проверкой корректности данных) был готов отчёт в Google Sheets.

#### Примечание
Я бы очень хотел реализовать полную автоматизацию для отчёта, запускать скрипт через Airflow (разобрался самостоятельно, как это можно было бы сделать), однако реализовать не получилось из-за не совсем верных данных финансовых, которые WB отдавал по API. 